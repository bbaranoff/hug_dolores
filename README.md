Interested, star her ! When sufficient stars then may i will make her open source.

# Dolores v5 LoRA

Ce d√©p√¥t contient le **script et le template** pour utiliser **Dolores v5 LoRA** avec LLaMA 3.1-Instruct. Le LoRA permet de sp√©cialiser le mod√®le sur tes donn√©es d‚Äôinstructions tout en gardant la base de LLaMA.

> ‚ö†Ô∏è Le mod√®le de base LLaMA 3.1 n‚Äôest **pas inclus** pour des raisons de licence. Vous devez le t√©l√©charger s√©par√©ment via Hugging Face.

---

## üì¶ Contenu du d√©p√¥t

* `run_dolores_template.py` : Script Python pour lancer le chat interactif avec template Jinja.
* `chat_template.jinja` : Template Jinja pour formater les prompts.
* `adapter_config.json` : Configuration du LoRA.
* `adapter_model.safetensors` : Poids LoRA (si le repo est priv√©).
* `README.md` : Ce fichier.

---

## üîß Pr√©requis

* Python ‚â• 3.12
* GPU recommand√© (RTX 4090 ou √©quivalent)
* Librairies Python :

```bash
pip install torch transformers peft jinja2
```

* Git LFS si tu r√©cup√®res le LoRA via Hugging Face :

```bash
git lfs install
```

---

## ‚ö° Installation depuis Hugging Face

1. Cloner le d√©p√¥t :

```bash
git clone https://huggingface.co/<votre-username>/dolores-v5-lora
cd dolores-v5-lora
```

2. Installer les fichiers LoRA (adapter_model.safetensors + adapter_config.json) :

```bash
mkdir -p ~/.ollama/models/dolores-v5/
cp adapter_model.safetensors ~/.ollama/models/dolores-v5/
cp adapter_config.json ~/.ollama/models/dolores-v5/
cp chat_template.jinja ~/.ollama/models/dolores-v5/
```

---

## üîπ Fusionner le LoRA avec le mod√®le de base

```bash
python merge_dolores.py
```

* Le mod√®le fusionn√© sera sauvegard√© dans :

```
/home/nirvana/dolores-v5-merged/
```

* Le dossier contient : `pytorch_model.bin`, `tokenizer.json`, `config.json` ‚Üí pr√™t pour l‚Äôinf√©rence.

---

## üó®Ô∏è Lancer le chat interactif

```bash
python run_dolores_template.py
```

Exemple :

```
Dolores v5 LoRA pr√™te avec template ! Tapez 'quit' pour sortir.

Vous: Bonjour Dolores
Dolores: Bonjour ! Comment puis-je vous aider aujourd'hui ?
```

* Tape `quit` ou `exit` pour fermer le chat.

---

## ‚öôÔ∏è Options et conseils

* **Dtype** : `bfloat16` recommand√© pour RTX 4090.
* **Tokens g√©n√©r√©s** : `max_new_tokens=200` recommand√© pour √©viter de saturer la VRAM.
* **GPU faible / CPU** : utiliser `load_in_8bit=True` dans `merge_dolores.py` pour √©conomiser de la m√©moire.

