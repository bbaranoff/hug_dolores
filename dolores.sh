#!/usr/bin/env bash
set -Eeo pipefail

# === RÃ©glages rapides ===
IMAGE="${IMAGE:-bastienbaranoff/dolores_v5}"
MODEL="${MODEL:-dolores}"
PORT="${PORT:-11434}"
VOLUME="${VOLUME:-ollama}"
SUDO_DEFAULT="${SUDO:-sudo}"
AUTO_GPU_LIMIT="${AUTO_GPU_LIMIT:-70}"
PULL_PROGRESS="${PULL_PROGRESS:-1}"

# Si root, pas besoin de sudo
if [ "$(id -u)" -eq 0 ]; then
  SUDO=""
else
  SUDO="$SUDO_DEFAULT"
fi

log()   { printf "\033[1;36m[+]\033[0m %s\n" "$*"; }
error() { printf "\033[1;31m[âœ–]\033[0m %s\n" "$*"; exit 1; }

trap 'log "âš ï¸  Une Ã©tape a Ã©chouÃ©, poursuite du script..."' ERR

log "=== Initialisation de Dolores (image: $IMAGE, modÃ¨le: $MODEL) ==="

# === Ã‰tape 1 : DÃ©pendances systÃ¨me ===
if command -v apt-get >/dev/null 2>&1; then
  export DEBIAN_FRONTEND=noninteractive
  log "Mise Ã  jour du cache apt..."
  $SUDO apt-get update -y
  log "Installation des paquets requis..."
  $SUDO apt-get install -y --no-install-recommends \
    ca-certificates curl gnupg lsb-release apt-transport-https \
    netcat-openbsd python3-venv
else
  error "apt-get non trouvÃ© â€” environnement non Debian/Ubuntu."
fi

# === Ã‰tape 2 : Docker ===
if ! command -v docker >/dev/null 2>&1; then
  log "Installation de Docker..."
  $SUDO install -m 0755 -d /etc/apt/keyrings
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | $SUDO gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
    https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable" \
    | $SUDO tee /etc/apt/sources.list.d/docker.list >/dev/null
  $SUDO apt-get update -y
  $SUDO apt-get install -y --no-install-recommends \
    docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
fi
$SUDO systemctl restart docker || true

# === Ã‰tape 3 : DÃ©tection GPU ===
GPU_FLAG=()
VRAM_GB=0
if command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi >/dev/null 2>&1; then
  log "GPU NVIDIA dÃ©tectÃ© â€” utilisation directe."
  GPU_FLAG=(--gpus all)
  RAW_VRAM=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -n1 || echo 0)
  VRAM_GB=$((RAW_VRAM / 1024))
  log "VRAM dÃ©tectÃ©e : ${VRAM_GB} Go"
else
  log "âš ï¸  Aucun GPU NVIDIA dÃ©tectÃ© â€” passage en mode CPU uniquement."
fi

# === Ã‰tape 4 : Ajustement du modÃ¨le selon VRAM ===
if (( VRAM_GB <= 0 )); then
  CONTEXT=2048; CACHE_TYPE="q4_0"
elif (( VRAM_GB <= 4 )); then
  CONTEXT=2048; CACHE_TYPE="q8_0"
elif (( VRAM_GB <= 8 )); then
  CONTEXT=4096; CACHE_TYPE="q8_0"
else
  CONTEXT=8192; CACHE_TYPE="f16"
fi

# === Ã‰tape 5 : Pull de lâ€™image ===
log "PrÃ©paration du conteneur $IMAGE..."
if [ "$PULL_PROGRESS" -eq 1 ]; then
  $SUDO docker pull "$IMAGE" || log "Image locale dÃ©jÃ  prÃ©sente."
else
  $SUDO docker pull -q "$IMAGE" || log "Image locale utilisÃ©e."
fi

# === Ã‰tape 6 : Lancement dâ€™Ollama en arriÃ¨re-plan ===
log "Lancement du serveur Ollama (port $PORT)..."
$SUDO docker run -d --rm "${GPU_FLAG[@]}" \
  -p "$PORT:$PORT" \
  -v "$VOLUME":/root/.ollama \
  -e OLLAMA_HOST="0.0.0.0:$PORT" \
  -e OLLAMA_KV_CACHE_TYPE="$CACHE_TYPE" \
  -e OLLAMA_NUM_PARALLEL=1 \
  -e OLLAMA_MAX_LOADED_MODELS=1 \
  "$IMAGE" \
  bash -lc "ollama serve" >/dev/null

# === Ã‰tape 7 : Attente du port 11434 (Ollama prÃªt) ===
log "â³ Attente que Ollama rÃ©ponde sur le port $PORT..."
for i in {1..30}; do
  if nc -z 127.0.0.1 "$PORT" >/dev/null 2>&1; then
    log "âœ… Ollama est prÃªt !"
    break
  fi
  sleep 2
  if [ "$i" -eq 30 ]; then
    error "â›” Ollama ne rÃ©pond pas aprÃ¨s 60 secondes."
  fi
done

# === Ã‰tape 8 : Bridge Flask (optionnel) ===
read -rp "âš™ï¸  Souhaitez-vous activer le bridge API Flask (Dolores â†” OpenAI) ? [y/N] " ENABLE_API
if [[ "$ENABLE_API" =~ ^[YyOo] ]]; then
  log "Activation du bridge Flask (API)..."

  read -rp "ğŸ” Fournir un jeton OpenAI (OPENAI_API_KEY) pour /api/openai ? [y/N] " USE_TOKEN
  if [[ "$USE_TOKEN" =~ ^[YyOo] ]]; then
    read -rs -p "ğŸ‘‰ Entrez votre OPENAI_API_KEY (commence par 'sk-') : " OPENAI_API_KEY_INPUT
    echo
    if [[ "$OPENAI_API_KEY_INPUT" == sk-* ]]; then
      export OPENAI_API_KEY="$OPENAI_API_KEY_INPUT"
      echo "ğŸ”‘ Jeton chargÃ© (â€¦${OPENAI_API_KEY_INPUT: -6})"
    else
      echo "âš ï¸ Jeton invalide â€” /api/openai dÃ©sactivÃ©."
      unset OPENAI_API_KEY
    fi
  fi

  log "Installation du bridge Flaskâ€¦"
  python3 -m venv /tmp/.env_dolores
  source /tmp/.env_dolores/bin/activate
  pip install --no-cache-dir flask requests openai > /dev/null

  # === Ã©criture du code Python ===
  cat > /tmp/server.py <<'PYCODE'
#!/usr/bin/env python3
import os, json, requests
from flask import Flask, request, Response

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

OLLAMA_HOST  = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "dolores")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL   = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

app = Flask(__name__)

def get_openai_client():
    if not OPENAI_API_KEY or OpenAI is None:
        return None
    return OpenAI(api_key=OPENAI_API_KEY)

def stream_openai(prompt):
    client = get_openai_client()
    if not client: return
    stream = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
    )
    for chunk in stream:
        delta = chunk.choices[0].delta
        if delta and delta.content:
            yield delta.content

def stream_ollama(prompt):
    url = f"{OLLAMA_HOST}/api/generate"
    data = {"model": OLLAMA_MODEL, "prompt": prompt, "stream": True}
    with requests.post(url, json=data, stream=True) as r:
        for line in r.iter_lines():
            if not line: continue
            try: j = json.loads(line.decode())
            except Exception: continue
            if "response" in j: yield j["response"]
            if j.get("done"): break

@app.route("/api/ollama", methods=["POST"])
def api_ollama():
    prompt = request.json.get("prompt", "")
    return Response(stream_ollama(prompt), mimetype="text/plain")

@app.route("/api/openai", methods=["POST"])
def api_openai():
    if not OPENAI_API_KEY or OpenAI is None:
        return Response("OpenAI API non configurÃ©e.", status=503)
    user_prompt = request.json.get("user_prompt", "")
    local_reply = request.json.get("local_reply", "")
    extra = request.json.get("extra_instruction", "")
    full = f"Question : {user_prompt}\nRÃ©ponse locale : {local_reply}\n{extra}"
    return Response(stream_openai(full), mimetype="text/plain")

@app.route("/")
def index():
    return "<h3>Bridge actif sur /api/ollama et /api/openai</h3>"

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080, threaded=True)
PYCODE

  nohup /tmp/.env_dolores/bin/python /tmp/server.py >/tmp/bridge.log 2>&1 &
  echo ""
  echo "ğŸŒ Vous pouvez maintenant ouvrir votre navigateur :"
  echo "   ğŸ‘‰ http://127.0.0.1:8080 ğŸ˜Š"
  echo ""
else
  log "Bridge Flask dÃ©sactivÃ© par lâ€™utilisateur."
fi

# === Ã‰tape finale : Interaction CLI ===
log "âœ… Tout est prÃªt. Ollama Ã©coute sur le port $PORT."
echo "Tapez : curl http://127.0.0.1:$PORT/api/tags"
